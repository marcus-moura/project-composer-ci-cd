{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Composer CI/CD","text":""},{"location":"#descricao","title":"Descri\u00e7\u00e3o","text":"<p>Este projeto automatiza o processo de implanta\u00e7\u00e3o de DAGs do Cloud Composer usando pipelines de CI/CD. Ele envolve a cria\u00e7\u00e3o de inst\u00e2ncias do Composer via Terraform, configura\u00e7\u00e3o de DAGs de exemplo e arquivos de teste utilizando pytest, e configura\u00e7\u00e3o de jobs de CI/CD para ambientes de desenvolvimento e produ\u00e7\u00e3o com github actions.</p>"},{"location":"#arquitetura-do-projeto","title":"Arquitetura do Projeto","text":""},{"location":"#infraestrutura-do-cloud-composer","title":"Infraestrutura do Cloud Composer","text":"<p>Toda a infraestrutura do Cloud Composer \u00e9 provisionada utilizando o Terraform, o que proporciona uma abordagem automatizada e escal\u00e1vel para configurar e gerenciar ambientes de fluxo de trabalho.</p>"},{"location":"#arquivo-maintf","title":"Arquivo <code>main.tf</code>","text":"<p>O arquivo <code>main.tf</code> cont\u00e9m a defini\u00e7\u00e3o dos recursos necess\u00e1rios para configurar o ambiente do Cloud Composer, incluindo a cria\u00e7\u00e3o do bucket do Cloud Storage, a configura\u00e7\u00e3o da conta de servi\u00e7o e a defini\u00e7\u00e3o do ambiente do Composer.</p> create_composer_instance/main.tf <pre><code>provider \"google\" {\nproject     = var.project_id\nregion      = var.region\n}\n\n# Create bucket composer\nresource \"google_storage_bucket\" \"create_bucket\" {\nname                        = var.bucket_name_composer\nlocation                    = var.region\nuniform_bucket_level_access = true\nforce_destroy               = false\n}\n\n# Create service account composer\nresource \"google_service_account\" \"sa_composer\" {\naccount_id   = var.sa_composer_name\ndisplay_name = \"Create Service Account for Composer Environment\"\n}\n\n# Attach role worker in service account composer\nresource \"google_project_iam_member\" \"attach_role_composer-worker\" {\nproject = var.project_id\nrole    = \"roles/composer.worker\"\nmember  = \"serviceAccount:${google_service_account.sa_composer.email}\"\n}\n\n# Create composer instance\nresource \"google_composer_environment\" \"cluster_config_composer\" {\nproject = var.project_id\nname   = var.composer_name\nregion = var.region\nprovider = google-beta\nlabels = {env = var.work_environ}\n\nstorage_config {\n    bucket  = google_storage_bucket.create_bucket.name\n    }\n\nconfig {\n\n    software_config {\n        image_version = var.image_version_composer\n        airflow_config_overrides = {\n            core-dags_are_paused_at_creation = \"True\"\n            secrets-backend                  =  \"airflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend\"\n            secrets-backend_kwargs           =  \"{'project_id': '${var.project_id}', 'connections_prefix':'airflow-connections', 'variables_prefix':'airflow-variables', 'sep':'-'}\"\n        }\n\n        env_variables = {\n            work_environ = var.work_environ\n        }\n    }\n\n    workloads_config {\n        scheduler {\n        cpu        = 1\n        memory_gb  = 2\n        storage_gb = 2\n        count      = 1\n        }\n        web_server {\n        cpu = 1\n        memory_gb = 2\n        storage_gb = 2\n        }\n\n        worker {\n            cpu = 1\n            memory_gb = 2\n            storage_gb = 2\n            min_count = 1\n            max_count = 3\n        }\n        triggerer {\n            cpu = 0.5\n            memory_gb = 0.5\n            count = 1\n        }\n    }\n    environment_size = \"ENVIRONMENT_SIZE_SMALL\"\n}\n}\n</code></pre>"},{"location":"#arquivo-backendtf","title":"Arquivo <code>backend.tf</code>","text":"<p>O arquivo <code>backend.tf</code> especifica a configura\u00e7\u00e3o do backend para o Terraform, indicando onde armazenar o estado do Terraform.</p> <pre><code>terraform {\n  backend \"gcs\" {\n    bucket  = \"personal-terraform-backend\"\n    prefix  = \"state\"\n  }\n}\n</code></pre> <p>Cria\u00e7\u00e3o do bucket</p> <p>\u00c9 importante destacar que para utilizar o backend \u00e9 necess\u00e1rio ter um bucket criado no Google Cloud Storage.</p>"},{"location":"#arquivos-variablestf-e-tfvars","title":"Arquivos <code>variables.tf</code> e <code>.tfvars</code>","text":"<p>Os arquivos <code>variables.tf</code> e <code>.tfvars</code> desempenham um papel fundamental na defini\u00e7\u00e3o e configura\u00e7\u00e3o das vari\u00e1veis utilizadas para a cria\u00e7\u00e3o da infraestrutura do Cloud Composer. Enquanto o arquivo <code>variables.tf</code> define as vari\u00e1veis e seus tipos, o arquivo <code>.tfvars</code> especifica os valores dessas vari\u00e1veis para cada ambiente, como desenvolvimento (dev) e produ\u00e7\u00e3o (prod).</p>"},{"location":"#arquivo-variablestf","title":"Arquivo <code>variables.tf</code>","text":"<p>No arquivo <code>variables.tf</code>, s\u00e3o declaradas todas as vari\u00e1veis que ser\u00e3o utilizadas no Terraform para configurar a infraestrutura do Cloud Composer. Essas vari\u00e1veis incluem informa\u00e7\u00f5es como o ID do projeto, a regi\u00e3o, o nome do bucket do Cloud Storage, o nome da conta de servi\u00e7o e outras configura\u00e7\u00f5es necess\u00e1rias.</p> Exemplo de declara\u00e7\u00e3o de vari\u00e1veis no arquivo <code>variables.tf</code> <pre><code>variable \"project_id\" {\ndescription = \"ID do projeto do Google Cloud\"\ntype        = string\n}\n\nvariable \"region\" {\ndescription = \"Regi\u00e3o do Google Cloud para implantar o ambiente do Composer\"\ntype        = string\n}\n\nvariable \"bucket_name_composer\" {\ndescription = \"Nome do bucket do Cloud Storage para o ambiente do Composer\"\ntype        = string\n}\n\n// Outras vari\u00e1veis necess\u00e1rias...\n</code></pre>"},{"location":"#arquivo-tfvars","title":"Arquivo <code>.tfvars</code>","text":"<p>J\u00e1 no arquivo <code>.tfvars</code>, s\u00e3o atribu\u00eddos os valores espec\u00edficos das vari\u00e1veis para cada ambiente, permitindo a personaliza\u00e7\u00e3o e a diferencia\u00e7\u00e3o entre ambientes, como desenvolvimento e produ\u00e7\u00e3o. Isso facilita a manuten\u00e7\u00e3o e o gerenciamento de m\u00faltiplos ambientes com configura\u00e7\u00f5es distintas.</p> <p>Exemplo de atribui\u00e7\u00e3o de valores de vari\u00e1veis para o ambiente de desenvolvimento:</p> create_composer_instance/dev.tfvars<pre><code>project_id            = \"meu-projeto-dev\"\nregion                = \"us-central1\"\nbucket_name_composer  = \"meu-bucket-dev\"\n</code></pre> <p>Exemplo de atribui\u00e7\u00e3o de valores de vari\u00e1veis para o ambiente de produ\u00e7\u00e3o:</p> create_composer_instance/prod.tfvars<pre><code>project_id            = \"meu-projeto-prod\"\nregion                = \"us-central1\"\nbucket_name_composer  = \"meu-bucket-prod\"\n</code></pre> <p>Substitua as vari\u00e1veis</p> <p>Ao utilizar o projeto, substitua os valores das vari\u00e1veis dos arquivos com extens\u00e3o <code>tfvars</code>.</p>"},{"location":"#testes-automatizados","title":"Testes Automatizados","text":"<p>Neste projeto, os testes automatizados s\u00e3o realizados utilizando a biblioteca pytest. Eles s\u00e3o essenciais para garantir a integridade e o funcionamento correto das DAGs (Directed Acyclic Graphs) que comp\u00f5em o fluxo de trabalho do Cloud Composer.</p>"},{"location":"#tipos-de-testes-realizados","title":"Tipos de Testes Realizados","text":"<p>Teste de Importa\u00e7\u00e3o: Este teste valida a integridade das DAGs verificando se h\u00e1 erros de importa\u00e7\u00e3o nos arquivos localizados no diret\u00f3rio <code>dags</code>. Ao garantir que as DAGs possam ser importadas corretamente, minimizamos a ocorr\u00eancia de erros durante a execu\u00e7\u00e3o das tarefas agendadas.</p> <p>tests/test_dag_import_errors.py<pre><code>import os\nimport pytest\nfrom airflow.models import DagBag\n\ndef get_import_errors():\n    dag_bag = DagBag(include_examples=False)\n\n    def strip_path_prefix(path):\n        return os.path.relpath(path, os.environ.get(\"AIRFLOW_HOME\"))\n\n    return [(None, None)] + [\n        (strip_path_prefix(k), v.strip()) for k, v in dag_bag.import_errors.items()\n    ]\n\n@pytest.mark.parametrize(\n    \"rel_path,rv\", get_import_errors(), ids=[x[0] for x in get_import_errors()]\n)\ndef test_import_erros(rel_path, rv):\n\n    if rel_path and rv:\n        raise Exception(f\"{rel_path} failed to import with message \\n {rv}\")\n</code></pre> Teste de Tags: O teste de tags verifica se todas as DAGs est\u00e3o adequadamente etiquetadas. As tags s\u00e3o metadados atribu\u00eddos \u00e0s DAGs que ajudam a organizar e categorizar as tarefas do fluxo de trabalho. Este teste garante que todas as DAGs estejam devidamente identificadas, facilitando a sua gest\u00e3o e manuten\u00e7\u00e3o.</p> tests/test_dag_tags.py<pre><code>import os\nimport pytest\nfrom airflow.models import DagBag\n\ndef get_dags():\n    dag_bag = DagBag(include_examples=False)\n\n    def strip_path_prefix(path):\n        return os.path.relpath(path, os.environ.get(\"AIRFLOW_HOME\"))\n\n    return [(k, v, strip_path_prefix(v.fileloc)) for k, v in dag_bag.dags.items()]\n\n@pytest.mark.parametrize(\n    \"dag_id,dag,fileloc\", get_dags(), ids=[x[2] for x in get_dags()]\n)\ndef test_dag_tags(dag_id, dag, fileloc):\n    assert dag.tags, f\"{dag_id} in {fileloc} has no tags\"\n</code></pre> <p>Esses testes automatizados s\u00e3o fundamentais para garantir a estabilidade e a confiabilidade do processo de implanta\u00e7\u00e3o automatizada das DAGs no ambiente do Cloud Composer. Eles contribuem para a detec\u00e7\u00e3o precoce de problemas e ajudam a manter a qualidade do c\u00f3digo ao longo do tempo.</p> <p>Poss\u00edveis problemas em ambientes Windows</p> <p>Em ambientes Windows, pode ocorrer de os testes n\u00e3o funcionarem corretamente devido a diferen\u00e7as de comportamento do Apache Airflow nesse sistema operacional. Um erro comum \u00e9 o <code>ModuleNotFoundError: No module named 'fcntl'</code>, que pode ser causado por incompatibilidades entre o Airflow e o ambiente Windows.</p> <p>Esses problemas s\u00e3o geralmente relacionados \u00e0 falta de suporte para certos recursos espec\u00edficos do sistema operacional Unix, como o m\u00f3dulo <code>fcntl</code>, que n\u00e3o est\u00e1 dispon\u00edvel no Windows.</p> <p>Recomenda-se verificar a documenta\u00e7\u00e3o do Apache Airflow para poss\u00edveis solu\u00e7\u00f5es alternativas ou considerar a execu\u00e7\u00e3o dos testes em um ambiente Unix-like, como Linux ou macOS, onde o Airflow \u00e9 mais comumente utilizado e testado.</p>"},{"location":"#fluxo-do-cicd","title":"Fluxo do CI/CD","text":""},{"location":"#pipeline-para-ambiente-de-desenvolvimento","title":"Pipeline para Ambiente de Desenvolvimento","text":"<p>O pipeline <code>pipeline-composer-dags-dsv.py</code> \u00e9 respons\u00e1vel por implantar as DAGs no ambiente de desenvolvimento. Ele \u00e9 acionado automaticamente em eventos de push para a branch <code>develop</code>, permitindo a implanta\u00e7\u00e3o de altera\u00e7\u00f5es em um ambiente isolado de produ\u00e7\u00e3o. Este pipeline \u00e9 desencadeado quando ocorrem altera\u00e7\u00f5es nos seguintes arquivos ou diret\u00f3rios:</p> <pre><code>on:\n  push:\n    branches: [\"develop\"]\n    paths:\n      - \"dags/*\"\n      - \"tests/*\"\n      - \"requirements.txt\"\n      - \"requirements-test.txt\"\n      - \".github/workflows/pipeline-composer-dags-dsv.yml\"\n  workflow_dispatch:\n</code></pre> <p>Este pipeline realiza as seguintes etapas:</p>"},{"location":"#1-clone-do-repositorio","title":"1. Clone do Reposit\u00f3rio","text":"<pre><code>jobs:\n  build-and-deploy:\n    steps:\n      - uses: actions/checkout@v4\n</code></pre>"},{"location":"#2-configuracao-do-ambiente-python-e-instalacao-de-dependencias","title":"2. Configura\u00e7\u00e3o do Ambiente Python e Instala\u00e7\u00e3o de Depend\u00eancias","text":"<pre><code>jobs:\n  build-and-deploy:\n    steps:\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n          pip install -r requirements-test.txt\n</code></pre>"},{"location":"#3-definicao-da-variavel-de-ambiente-airflow_home","title":"3. Defini\u00e7\u00e3o da Vari\u00e1vel de Ambiente <code>AIRFLOW_HOME</code>","text":"<pre><code>jobs:\n  build-and-deploy:\n    steps:\n      - name: Set AIRFLOW_HOME\n        run: echo \"AIRFLOW_HOME=${AIRFLOW_HOME}\" &gt;&gt; $GITHUB_ENV\n</code></pre>"},{"location":"#4-execucao-dos-testes-unitarios-nas-dags","title":"4. Execu\u00e7\u00e3o dos Testes Unit\u00e1rios nas DAGs","text":"<pre><code>jobs:\n  build-and-deploy:\n    steps:\n      - name: Run tests\n        run: |\n          python -m pytest tests/\n</code></pre>"},{"location":"#5-autenticacao-no-google-cloud","title":"5. Autentica\u00e7\u00e3o no Google Cloud","text":"<pre><code>jobs:\n  deploy-to-gcs:\n    steps:\n      - uses: google-github-actions/auth@v2\n        with:\n          credentials_json: ${{ secrets.GCP_SA_KEY }}\n</code></pre>"},{"location":"#6-configuracao-do-cloud-sdk","title":"6. Configura\u00e7\u00e3o do Cloud SDK","text":"<pre><code>jobs:\n  deploy-to-gcs:\n    steps:\n      - uses: google-github-actions/setup-gcloud@v2\n</code></pre>"},{"location":"#7-upload-das-dags-para-o-cloud-storage","title":"7. Upload das DAGs para o Cloud Storage","text":"<pre><code>jobs:\n  deploy-to-gcs:\n    steps:\n      - run: |\n          gsutil -m rsync -r -x \"__pycache__\" dags ${{ env.DAGS_BUCKET_PATH }}\n</code></pre>"},{"location":"#pipeline-para-ambiente-de-producao","title":"Pipeline para Ambiente de Produ\u00e7\u00e3o","text":"<p>O pipeline <code>pipeline-composer-dags-prd.py</code> \u00e9 respons\u00e1vel por implantar as DAGs no ambiente de produ\u00e7\u00e3o. Ele \u00e9 acionado automaticamente em eventos de pull request para a branch <code>main</code>, garantindo que apenas as altera\u00e7\u00f5es validadas sejam implantadas no ambiente de produ\u00e7\u00e3o. Este pipeline \u00e9 desencadeado quando ocorrem altera\u00e7\u00f5es nos seguintes arquivos ou diret\u00f3rios:</p> <pre><code>on:\n  pull_request:\n    branches: [\"main\"]\n    paths:\n      - \"dags/*\"\n      - \"tests/*\"\n      - \"requirements.txt\"\n      - \"requirements-test.txt\"\n      - \".github/workflows/pipeline-composer-dags-prd.yml\"\n</code></pre> <p>Este pipeline realiza as seguintes etapas:</p>"},{"location":"#1-clone-do-repositorio_1","title":"1. Clone do Reposit\u00f3rio","text":"<pre><code>jobs:\n  deploy-to-gcs:\n    steps:\n      - uses: actions/checkout@v4\n</code></pre>"},{"location":"#5-autenticacao-no-google-cloud-e-configuracao-do-cloud-sdk","title":"5. Autentica\u00e7\u00e3o no Google Cloud e Configura\u00e7\u00e3o do Cloud SDK","text":"<p>(Autentica\u00e7\u00e3o e configura\u00e7\u00e3o s\u00e3o semelhantes aos passos do pipeline de produ\u00e7\u00e3o)</p>"},{"location":"#6-upload-das-dags-para-o-cloud-storage","title":"6. Upload das DAGs para o Cloud Storage","text":"<p>(Semelhante ao passo de upload no pipeline de produ\u00e7\u00e3o)</p> <p>Configura\u00e7\u00e3o de Segredo e Vari\u00e1veis no GitHub Actions</p> <p>Para o funcionamento correto do CI/CD certifique-se de criar uma service account espec\u00edfica para o projeto no Google Cloud Platform e armazen\u00e1-la no secret do GitHub Actions. Isso \u00e9 essencial para garantir que o pipeline de CI/CD tenha acesso \u00e0s credenciais necess\u00e1rias para interagir com os recursos do GCP. Al\u00e9m disso, n\u00e3o se esque\u00e7a de definir as vari\u00e1veis de ambiente <code>DAGS_BUCKET_PATH_DEV</code> e <code>DAGS_BUCKET_PATH_PROD</code> no GitHub Actions, para especificar os caminhos dos buckets onde as DAGs ser\u00e3o armazenadas nos ambientes de desenvolvimento e produ\u00e7\u00e3o, respectivamente.</p>"},{"location":"#consideracoes-finais","title":"Considera\u00e7\u00f5es finais","text":"<p>Leia o README do Reposit\u00f3rio</p> <p>Para obter informa\u00e7\u00f5es sobre como instalar e utilizar o projeto, n\u00e3o deixe de ler o README no reposit\u00f3rio oficial do projeto.</p> <p>Esta documenta\u00e7\u00e3o fornece uma vis\u00e3o abrangente do projeto, abordando aspectos como a arquitetura, a infraestrutura, os testes automatizados, os fluxos de CI/CD e muito mais. Esperamos que este guia seja \u00fatil para entender e utilizar o projeto de forma eficaz.</p> <p>Se voc\u00ea tiver alguma d\u00favida, problema ou sugest\u00e3o, n\u00e3o hesite em entrar em contato.</p>"}]}